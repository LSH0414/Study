{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 분류"
      ],
      "metadata": {
        "id": "dQl0PSI6XUoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 분류 모델(Classification Model)\n",
        "　결과값이 연속된 값이 아닌 범주형인 경우 분류 모델을 사용한다. (ex. 타이타닉 생존자 예측)\n",
        "\n",
        "- 대표적 알고리즘\n",
        "  - Logistic : 독립변수와 종속변수의 선형 관계성에 기반\n",
        "  - SVM(Support Vector Machine) : 개별 클래스간의 최대 분류 마진을 효과적으로 찾음\n",
        "  - Decision Tree : 데이터 균일도에 따른 규칙 기반\n",
        "  - Navie Bayes : 베이즈 통계와 생성 모델에 기반\n",
        "  - KNN(k-Nearest-Neighbor) : 근접거리를 기준으로 하는 최소 근접 알고리즘\n",
        "  - NN(Neural Network) : 심층 연결 기반 신경망\n",
        "  - Ensemble : 서로 다른 혹은 같은 머신러닝 알고리즘을 결합\n",
        "\n",
        "</br></br></br>\n",
        "### KNN(k-Nearest-Neighbor)\n",
        "　선택한 한 점으로부터 가까운 거리에 있는 k개의 점을 선택한 후, k개의 점들이 가장 많이 속한 클래스를 찾아 선택한 점이 그 클래스(그룹)에 속한다고 평가하는 알고리즘\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/119479455/223959104-2284b89c-2438-4bde-a070-d2bf6706b561.png)\n",
        "\n",
        "</br>\n",
        "- 한계점 : 훈련 데이터가 충분히 많으면 좋은 성능을 발휘하지만 차원(Feature)가 증가하면 성능이 저하된다. 또한 차원이 증가할수록 고차원 공간을 채울 데이터가 많이 필요하고 멀리 떨어진 훈련데이털르 참조해야하기 때문에 '근접이웃'에 한정하기 어렵다.\n",
        "</br></br></br>\n",
        "\n",
        "### Decision Tree(의사 결정 나무)\n",
        "　데이터에 있는 규칙을 학습하여 자동으로 트리 기반의 분류 규칙을 생성.\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/119479455/223959190-32798873-29fc-4584-9282-8be6b00e9d63.png)\n",
        "</br>\n",
        "분류나 예측의 근거를 알 수 있어 결정 과정을 쉽게 이해할 수 있다. 또한 데이터의 Feature 많아져도 분류에 중요하지 않은 Feature를 제외할 수 있어 선정 단계에서 크게 신경 쓸 필요가 없다.</br></br>\n",
        "트리 모형을 과하게 분할할 경우 과적합 문제가 발생한다. 이를 위해서 가지치기 방법을 사용한다.</br></br>가지치기 방법에는 사전 가지치기와 사후 가지치기가 있다. 사전 가지치기는 정치 규칙을 만든 후 분할을 시작한다. 사후 가지치기는 초기에 트리를 최대 크기로 만들고 마지막 노드의 불순 척도가 최소가 되도록 분할한다. \n",
        "\n",
        "</br></br>\n",
        "- 불순척도(impurity)</br>\n",
        "　데이터 분할 시, 분할 전보다 후에 불순한 정도가 줄어드는 것이 좋다고 가정한다. '불순'이란 데이터가 섞인 정도이다.</br>\n",
        "이질적인 데이터가 많을수록 해당 노드에서 불순도가 높다고 한다.</br>\n",
        "부모 노드에서 자식 노드로 갈 때 불순도는 최대한 감소하도록 해야한다.\n",
        "</br></br>\n",
        "- 지니지수\n",
        "$${Gini(t) = 1 - Σ^c_{i=1}p(i|t)^2 }$$\n",
        "　지니지수 혹은 지니계수는 **불순도**를 측정하는 지표로 데이터의 **통계적 분산**정도를 정량화하여 표현한 값이다.\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/119479455/223967193-68bb1656-1db7-46a0-8cd0-28e205353c07.png)\n",
        "\n",
        "</br></br>\n",
        "\n",
        "- 엔트로피\n",
        "$${H(t) = -Σ^c_{i=1}p(i|t)log_2p(i|t)}$$\n",
        "　무질서도를 정량화해서 표현한 값. 어떤 집합의 엔트로피가 높을 수록 그 집단의 특성을 찾는 것이 어렵다는 것을 의미한다. 의사 결정 나무의 자식 노드들의 엔트로피가 최소가 되는 방향으로 분류해 나가는 것이 최적의 방법으로 분류한 것이라 할 수 있다.\n",
        "</br></br>\n",
        "- 정보 획득(information gain)</br>\n",
        "　정보이론의 핵심은 잘 일어나지 않는 사건은 자주 발생하는 사건보다 정보량이 많다는 것.\n",
        "\n",
        "![img](https://upload.wikimedia.org/wikipedia/ko/7/71/Entropy_kor.jpg)\n",
        "</br></br>\n",
        "- 최적의 분할 선태 기준\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/119479455/223967815-726e0d3a-e0c4-4bfd-9c99-31cbace9bb39.png)\n",
        "\n",
        "</br></br></br>\n",
        "### Ensemble & Boosting\n",
        "　**앙상블(Ensemble)**\n",
        "　　여러 개의 분류기를 생성하고 그 예측을 결합함으로써 보다 정확한 최종 예측을 도출하는 기법 앙상블 기법에는 Voting과 Bagging이 있다.\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/119479455/223975650-53dc9b1e-de9e-403e-be7c-0b711fba2fe2.png)\n",
        "\n",
        "위 그림과 같이 Voting은 같은 데이터로 서로 다른 학습기를 사용하여 결과를 도출한다. 반면 Bagging은 다른 훈련 데이터로 같은 학습기를 사용하여 결과를 도출한다.\n",
        "\n",
        "- 배깅(Bagging)</br>\n",
        "　Bootstrap(단순복원 임의 추출)을 통해서 데이터 셋을 만들어낸다. 만들어진 데이터셋은 원본 훈련 데이터와 같은 크기를 갖고 중복을 허용한다.\n",
        "</br>\n",
        "- 랜덤 포레스트(Random Forest)</br>\n",
        "　의사 결정 나무를 사용한 앙상블 기법이다. 배깅을 사용해 데이터 셋을 만들어내고 데이터 셋마다 의사 결정 나무를 구축한다. 각 트리는 다른 조금씩 다른 결과를 보여주는데 이를 종합하여 결과를 낸다. \n",
        "\n",
        "![img](https://user-images.githubusercontent.com/119479455/223975750-98de4c5b-69e2-47d1-b90a-1602f278d110.png)\n",
        "</br></br>\n",
        "- 부스팅(Boosting)</br>\n",
        "　배깅과 같은 방법으로 훈련 데이터 셋을 생성한다. 다른점은 모든 데이터에 가중치가 부여되는데 잘못 분류된 데이터에 대해서 가중치가 높아져 성능을 개선하는 방식이다.\n",
        "</br></br>\n",
        "- AdoBoost</br>\n",
        "　여러 개의 약한 분류기를 사용하고, 잘못 분류한 데이터 샘플을 더 많이 샘플링해서 정확도를 높이려는 알고리즘\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/119479455/223975880-7e4b9857-2d1f-4d5c-9054-0d44eedfa496.png)\n",
        "</br></br>\n",
        "- GBM(Gradient Boosting Machine)</br>\n",
        "　AdoBoost와 유사하다. 다만 가중치 계산을 경사하강법을 사용하여 업데이트한다.\n",
        "\n",
        "</br></br>\n",
        "- XGBoost(eXtreme Gradient Boost)</br>\n",
        "　GBM을 병렬처리가 가능하도로 만든 모델. 병렬처리로 인해 속도가 GBM에 빠를뿐 아니라 규제와 가지치기를 통해 과적합을 줄이고 일반화 특성을 좋게 한다.\n",
        "\n",
        "</br></br>\n",
        "- LightGBM</br>\n",
        "　GBM에 기반한 모델. 다른점은 처리 방식에 있는데 level이 아닌 leaf 중심으로 확장된다. 즉 수직적 확장이 아닌 수평적 확장이다. </br></br>이름에 Light가 있듯 빠른 속도를 자랑하며 GPU도 지원한다.\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/119479455/223975983-4ab259da-ed19-4367-b4cb-6cfceeb57c40.png)"
      ],
      "metadata": {
        "id": "jgcQLiKmXb6f"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bjhMLVLIYlcP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}