{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "trWWdrnwz2Re"
   },
   "source": [
    "# MultiProcessing in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-Hw0KcbIduMt"
   },
   "source": [
    "## 멀티 프로세싱\n",
    "\n",
    "### 멀티프로세스란?\n",
    "우선 프로세스(Processes)란 **실행 중인 프로그램**을 의미합니다. 그렇다면 멀티 프로세스란 이 프로세스를 여러개 사용한다는 의미가 됩니다. 즉, 분산처리 개념으로 이해하면 될 것 같습니다.</br>\n",
    "그러면 이 방법을 왜 사용할까요? 간단한 예시로 스테이크와 스테이크 소스 두 가지를 만들어야 하는 상황일때 사용할 수 있는 가스레인지 모델이 1구인 모델과 2구인 모델의 음식 완성까지의 시간은 다르겠죠? 컴퓨터에서 하는 작업도 같은 맥락으로 이해하시면 됩니다.</br></br>\n",
    "\n",
    "Python에서는 **multiprocessing**이라는 모듈이 내장되어 있어 이를 쉽게 구현할 수 있다고 합니다.\n",
    "\n",
    "</br>\n",
    "\n",
    "크롤링에 적용하기 이전에 간단한 예제로 멀티프로세싱에 대해서 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dbr47Jt8eTKW"
   },
   "source": [
    "#### 간단한 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgtaUvV0eVoA",
    "outputId": "d186524f-8907-4556-c340-cd48f39f4711"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 sec Sleep....\n",
      "Sleep Fin 1sec\n",
      "2 sec Sleep....\n",
      "Sleep Fin 2sec\n",
      "3 sec Sleep....\n",
      "Sleep Fin 3sec\n",
      "4 sec Sleep....\n",
      "Sleep Fin 4sec\n",
      "Total Running Time :  10.013209819793701\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def sleepFunction(t):\n",
    "  print(f\"{t} sec Sleep....\")\n",
    "  time.sleep(t)\n",
    "  print(f'Sleep Fin {t}sec')\n",
    "\n",
    "num_list = [1,2,3,4]\n",
    "\n",
    "\n",
    "for num in num_list:\n",
    "  sleepFunction(num)\n",
    "\n",
    "print('Total Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ww6gqeys1m7b"
   },
   "source": [
    "위 예시에서 보시다시피 리스트를 단순 반복문을 돌리면 앞에 작업이 끝나기 전에는 뒷 작업을 진행  할 수 없어 총 대기시간의 합인 10초가 걸렸습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JR-8bt4ge0-e",
    "outputId": "8aee2c02-0948-40f2-dcf0-5086d797ac6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 sec Sleep....3 sec Sleep....2 sec Sleep....\n",
      "4 sec Sleep....\n",
      "\n",
      "\n",
      "Sleep Fin 1sec\n",
      "Sleep Fin 2sec\n",
      "Sleep Fin 3sec\n",
      "Sleep Fin 4sec\n",
      "Total Running Time :  4.073814630508423\n"
     ]
    }
   ],
   "source": [
    "# 멀티프로세싱\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def sleepFunction(t):\n",
    "  print(f\"{t} sec Sleep....\")\n",
    "  time.sleep(t)\n",
    "  print(f'Sleep Fin {t}sec')\n",
    "\n",
    "num_list = [1,2,3,4]\n",
    "\n",
    "pool = Pool(processes=4)\n",
    "pool.map(sleepFunction, num_list)\n",
    "\n",
    "print('Total Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OUJ2PQv7f5KG"
   },
   "source": [
    "멀티프로세싱으로 4초의 시간만 걸린 것을 확인 할 수 있습니다. 앞서 스테이크 예시와 같이 작업할 수 있는 공간이 늘어났기 때문에 시간이 줄어든것입니다.\n",
    "</br></br>\n",
    "이 멀티프로세싱에는 pool과 process 두 가지 방법이 있다고 합니다.</br>pool은 작업 전체를 던져주고 알아서 처리해! 라는 방식이고 process는 직접 넌 이거 넌 저거 이런식으로 지정해주는 것이라고합니다. 아래는 위 작업을 process로 구현한 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4eFine1k6kA",
    "outputId": "2cad4719-cc01-4e62-b336-43bb86aac655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 sec Sleep....\n",
      "1 sec Sleep....\n",
      "3 sec Sleep....4 sec Sleep....\n",
      "\n",
      "Sleep Fin 1sec\n",
      "Sleep Fin 2sec\n",
      "Sleep Fin 3sec\n",
      "Sleep Fin 4sec\n",
      "Total Running Time :  4.109197378158569\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as redfoxtistory\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def sleepFunction(t):\n",
    "  sec = int(t)\n",
    "  print(f\"{sec} sec Sleep....\")\n",
    "  time.sleep(sec)\n",
    "  print(f'Sleep Fin {sec}sec')\n",
    "\n",
    "p1 = redfoxtistory.Process(target = sleepFunction, args = ('1'))\n",
    "p2 = redfoxtistory.Process(target = sleepFunction, args = ('2'))\n",
    "p3 = redfoxtistory.Process(target = sleepFunction, args = ('3'))\n",
    "p4 = redfoxtistory.Process(target = sleepFunction, args = ('4'))\n",
    "\n",
    "#작업대에서 해야할 일을 설정해주고 실행시켜줍니다.\n",
    "p1.start()\n",
    "p2.start()\n",
    "p3.start()\n",
    "p4.start()\n",
    "\n",
    "# join은 저희가 기존에 사용하던 str.join()과 완전 별개의 함수입니다.\n",
    "# multiprocessing안에 있는 함수로 이 함수는 내가 할당한 작업대의 작업이 끝날때까지 기다려라 라는 뜻입니다.\n",
    "# 아래 부분을 모두 활성화 해도 합이 아닌 가장 시간이 오래걸리는 작업대 시간만큼만 기다리는거보면 큰 작업장 개념이 따로 있나봅니다.\n",
    "# 좀 더 쉽게 이해하고 싶으시면 p3만 활성화 하고 실행해보시면 p4는 작업을 완료하지 못했는데 그냥 넘어가버립니다.\n",
    "# 다른 방법으로는 start와 조인을 번갈아 사용해보시면 됩니다. start -> join 하면 이 작업이 끝나야 다음 start가 실행됩니다.\n",
    "\n",
    "p1.join() # 1초 대기\n",
    "p2.join() # 2초 대기\n",
    "p3.join() # 3초 대기\n",
    "p4.join() # 4초 대기\n",
    "\n",
    "print('Total Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXLIItJ_l05j"
   },
   "source": [
    "#### 크롤링에 적용해보기\n",
    "\n",
    "우선 기본적인 뷰티풀숩으로 0201일의 경제-금융의 기사들을 크롤링해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YTrvArhG1IUP"
   },
   "outputs": [],
   "source": [
    "# 저는 useragent를 페이크로 사용했습니다. 직접 입력해주시면 이부분은 실행 안하셔도 됩니다.\n",
    "!pip install fake_useragent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0qKGYJjh0I3y"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "\n",
    "#useragent를 직접 입력하신다면 아래부분은 지우시고 headers부분만 직접 입력해주시면 됩니다.\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent(verify_ssl=False)\n",
    "fake_ua = ua.random\n",
    "\n",
    "headers = {\n",
    "    'user-agent' : fake_ua\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UrhLNw-Y0LQJ",
    "outputId": "7878ff27-91b2-485a-b52a-0f9b104c0a10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 PAGE FIN URLS CNT :  20\n",
      "2 PAGE FIN URLS CNT :  20\n",
      "3 PAGE FIN URLS CNT :  20\n",
      "4 PAGE FIN URLS CNT :  20\n",
      "5 PAGE FIN URLS CNT :  20\n",
      "6 PAGE FIN URLS CNT :  20\n",
      "7 PAGE FIN URLS CNT :  20\n",
      "8 PAGE FIN URLS CNT :  20\n",
      "9 PAGE FIN URLS CNT :  20\n",
      "10 PAGE FIN URLS CNT :  20\n",
      "11 PAGE FIN URLS CNT :  20\n",
      "12 PAGE FIN URLS CNT :  20\n",
      "13 PAGE FIN URLS CNT :  20\n",
      "14 PAGE FIN URLS CNT :  20\n",
      "15 PAGE FIN URLS CNT :  20\n",
      "16 PAGE FIN URLS CNT :  20\n",
      "17 PAGE FIN URLS CNT :  20\n",
      "18 PAGE FIN URLS CNT :  20\n",
      "19 PAGE FIN URLS CNT :  20\n",
      "20 PAGE FIN URLS CNT :  20\n",
      "21 PAGE FIN URLS CNT :  20\n",
      "22 PAGE FIN URLS CNT :  20\n",
      "23 PAGE FIN URLS CNT :  20\n",
      "24 PAGE FIN URLS CNT :  20\n",
      "25 PAGE FIN URLS CNT :  20\n",
      "26 PAGE FIN URLS CNT :  20\n",
      "27 PAGE FIN URLS CNT :  18\n",
      "----------\n",
      "END PAGE :  27\n",
      "Total Urls :  538\n",
      "Check Running Time :  14.20963191986084\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "i=1\n",
    "total_urls = []\n",
    "urls = []\n",
    "while True:\n",
    "  test_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page={i}'\n",
    "  soup = bs(requests.get(test_url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "  now_url =[]\n",
    "  # url만 가져오기\n",
    "  for row in soup.select('#main_content > div.list_body.newsflash_body > ul > li'):\n",
    "    row = row.select_one('a')\n",
    "    now_url.append(row['href'])\n",
    "  \n",
    "  if urls ==  now_url:\n",
    "    break\n",
    "  \n",
    "  urls = now_url\n",
    "  total_urls += now_url\n",
    "\n",
    "  print(f\"{i} PAGE FIN URLS CNT : \", len(urls))\n",
    "  i += 1\n",
    "\n",
    "print(\"-\"*10)\n",
    "print('END PAGE : ', i-1)\n",
    "print('Total Urls : ', len(total_urls))\n",
    "print('Check Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQZlRMaS0VmW"
   },
   "source": [
    "#### 멀티프로세싱을 통한 크롤링\n",
    "\n",
    "이번에는 멀티프로세싱으로 크롤링 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkWwvvJBo2ZY",
    "outputId": "2dde8cc5-9e3d-4c62-8279-58d8040026ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawing Fin 3 URLS CNT :  20\n",
      "Crawing Fin 7 URLS CNT :  20\n",
      "Crawing Fin 1 URLS CNT :  20\n",
      "Crawing Fin 5 URLS CNT :  20\n",
      "Crawing Fin 4 URLS CNT :  20\n",
      "Crawing Fin 8 URLS CNT :  20\n",
      "Crawing Fin 6 URLS CNT :  20\n",
      "Crawing Fin 2 URLS CNT :  20\n",
      "Crawing Fin 9 URLS CNT :  20\n",
      "Crawing Fin 11 URLS CNT :  20\n",
      "Crawing Fin 13 URLS CNT :  20\n",
      "Crawing Fin 15 URLS CNT :  20\n",
      "Crawing Fin 10 URLS CNT :  20\n",
      "Crawing Fin 12 URLS CNT :  20\n",
      "Crawing Fin 17 URLS CNT :  20\n",
      "Crawing Fin 19 URLS CNT :  20\n",
      "Crawing Fin 14 URLS CNT :  20\n",
      "Crawing Fin 18 URLS CNT :  20\n",
      "Crawing Fin 16 URLS CNT :  20\n",
      "Crawing Fin 20 URLS CNT :  20\n",
      "Crawing Fin 21 URLS CNT :  20\n",
      "Crawing Fin 23 URLS CNT :  20\n",
      "Crawing Fin 27 URLS CNT :  Crawing Fin 25 URLS CNT : 18\n",
      " 20\n",
      "Crawing Fin 22 URLS CNT :  20\n",
      "Crawing Fin 24 URLS CNT :  20\n",
      "Crawing Fin 26 URLS CNT :  20\n",
      "----------\n",
      "URL CNT :  538\n",
      "Check Running Time :  6.2536797523498535\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# 페이지 탐색을 자동으로 종료하는 방법은 떠오르지 않아 마지막 페이지 정보를 먼저 가져오도록 했습니다.\n",
    "# 위 방법처럼 url들을 비교하는 방법이 있을 것 같긴한데 저도 좀 더 알아봐야할 것 같습니다. 혹시 발견하신 분은 공유해주시면 좋을 것 같아요!\n",
    "def getEndPage():\n",
    "  url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page=999'\n",
    "  soup = bs(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "  return soup.select_one('#main_content > div.paging > strong').text\n",
    "  \n",
    "\n",
    "def getUrls(page):\n",
    "  test_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page={page}'\n",
    "  \n",
    "  res = requests.get(test_url, headers=headers)\n",
    "\n",
    "  if res.status_code != 200:\n",
    "    print(page, \"page Request Error\")\n",
    "    return \n",
    "  \n",
    "  soup = bs(res.text, \"html.parser\")\n",
    "  now_urls =[]\n",
    "  \n",
    "  for row in soup.select('#main_content > div.list_body.newsflash_body > ul > li'):\n",
    "      row = row.select_one('a')\n",
    "      now_urls.append(row['href'])  \n",
    "  \n",
    "  print(f\"Crawing Fin {page} URLS CNT : \", len(now_urls))\n",
    "  return now_urls\n",
    "\n",
    "pages = [i for i in range(1, int(getEndPage())+1)]\n",
    "\n",
    "pool = Pool(processes=4)\n",
    "result = pool.map(getUrls, pages)\n",
    "\n",
    "urls = []\n",
    "for url in result:\n",
    "  urls += url\n",
    "\n",
    "\n",
    "print(\"-\"*10)\n",
    "print('URL CNT : ', len(urls))\n",
    "print('Check Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ku5rDMAyxyt5"
   },
   "source": [
    "핵심인 시간이 6초로 크게 줄어든게 눈에 보입니다. 그렇다면 작업대(processes)를 2배로 늘려보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jpk5MVw4yK4z",
    "outputId": "0468d6f4-5b95-474f-b5c5-3dbe5d83d5b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawing Fin 2 URLS CNT : Crawing Fin 8 URLS CNT : Crawing Fin 7 URLS CNT :  Crawing Fin 1 URLS CNT :   2020 \n",
      "Crawing Fin 6 URLS CNT : 20\n",
      " \n",
      "20\n",
      "20Crawing Fin 3 URLS CNT : \n",
      " Crawing Fin 4 URLS CNT :  2020\n",
      "\n",
      "Crawing Fin 5 URLS CNT : 20 \n",
      "Crawing Fin 10 URLS CNT :  20\n",
      "Crawing Fin 11 URLS CNT : Crawing Fin 9 URLS CNT :   2020\n",
      "Crawing Fin 16 URLS CNT : Crawing Fin 12 URLS CNT :  \n",
      " 2020\n",
      "Crawing Fin 14 URLS CNT : \n",
      " 20\n",
      "Crawing Fin 15 URLS CNT :  Crawing Fin 13 URLS CNT : 20 20\n",
      "\n",
      "Crawing Fin 17 URLS CNT :  20\n",
      "Crawing Fin 18 URLS CNT :  20\n",
      "Crawing Fin 21 URLS CNT : Crawing Fin 19 URLS CNT :   20\n",
      "20\n",
      "Crawing Fin 24 URLS CNT :  Crawing Fin 22 URLS CNT : Crawing Fin 20 URLS CNT : 20Crawing Fin 23 URLS CNT :    2020\n",
      "\n",
      "\n",
      "20\n",
      "Crawing Fin 25 URLS CNT :  20\n",
      "Crawing Fin 26 URLS CNT :  20\n",
      "Crawing Fin 27 URLS CNT :  18\n",
      "----------\n",
      "URL CNT :  538\n",
      "Check Running Time :  4.952283620834351\n"
     ]
    }
   ],
   "source": [
    "# 위에서 작업했던 크롤링을 가져와서 그대로 사용해 보겠습니다.\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "def getEndPage():\n",
    "  url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page=999'\n",
    "  soup = bs(requests.get(url, headers=headers).text, \"html.parser\")\n",
    "\n",
    "  return soup.select_one('#main_content > div.paging > strong').text\n",
    "  \n",
    "\n",
    "def getUrls(page):\n",
    "  test_url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2=259&sid1=101&date=20230201&page={page}'\n",
    "  \n",
    "  res = requests.get(test_url, headers=headers)\n",
    "\n",
    "  if res.status_code != 200:\n",
    "    print(page, \"page Request Error\")\n",
    "    return \n",
    "  \n",
    "  soup = bs(res.text, \"html.parser\")\n",
    "  now_urls =[]\n",
    "  \n",
    "  for row in soup.select('#main_content > div.list_body.newsflash_body > ul > li'):\n",
    "      row = row.select_one('a')\n",
    "      now_urls.append(row['href'])  \n",
    "  \n",
    "  print(f\"Crawing Fin {page} URLS CNT : \", len(now_urls))\n",
    "  return now_urls\n",
    "\n",
    "pages = [i for i in range(1, int(getEndPage())+1)]\n",
    "\n",
    "pool = Pool(processes=8)\n",
    "result = pool.map(getUrls, pages)\n",
    "\n",
    "urls = []\n",
    "for url in result:\n",
    "  urls += url\n",
    "\n",
    "\n",
    "print(\"-\"*10)\n",
    "print('URL CNT : ', len(urls))\n",
    "print('Check Running Time : ', time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqiQNRWZyRfc"
   },
   "source": [
    "더 줄었네요! 그런데 여기서 중요한 점은 'processes'옵션의 숫자는 사용자의 컴퓨터의 성능에 따라 달라진다고 합니다.</br></br>\n",
    "컴퓨터 하드웨어 중 CPU를 평가할때 8코어 16코어 이렇게 이야기하는건 광고에서 강조하는 부분이라 한 번쯤은 들어보셨을거라 생각합니다. 이게 바로 프로세스를 처리할 수 있는 작업대 수를 결정하는 거라고 합니다. 따라서 작업대를 너무 많이 설정하면 오히려 속도 저하의 원인이 된다고 합니다.\n",
    "\n",
    "[멀티프로세스](https://superfastpython.com/multiprocessing-pool-num-workers/#What_is_a_CPU_and_What_is_a_CPU_Core)에 관련한 내용이 정리된 링크 공유로 마무리하겠습니다!</br>\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
