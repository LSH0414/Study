{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Scrapy 사용\n",
        "\n",
        "**저는 colab에서 작업했습니다.**\n",
        "\n",
        "pwd, cd명령어를 활용해서 시작할 위치를 잡아주시면 됩니다.\n",
        "\n",
        "- pwd : 현재 위치\n",
        "- cd [이동할 폴더명] : 위치 이동"
      ],
      "metadata": {
        "id": "6XqRVkXKBotn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install scrapy"
      ],
      "metadata": {
        "id": "va6oYzeOK4Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd 명령어를 사용해서 scrapy를 만들 장소(폴더)로 이동합니다.\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmXNSFVTQ0Vu",
        "outputId": "1b6a07b9-4ec9-47a8-a8c8-21d51baf5b8e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# scrapy 초기화(생성) startproject [이름 입력]\n",
        "# scrapy 프로젝트 만드는 과정입니다. 이름은 자유!\n",
        "!scrapy startproject navernews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-85kT8vxR80-",
        "outputId": "86fcd275-b807-41bd-b03d-3841f3630672"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'navernews', using template directory '/usr/local/lib/python3.9/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/navernews\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd navernews\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spider폴더로 이동해야 하므로 cd사용해서 이동해줍니다. cd ./폴더명/폴더명/spiders\n",
        "%cd ./navernews/navernews/spiders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzbyiP3Ddxq8",
        "outputId": "6721f48b-07f5-4fcb-fdfd-6246aadec4f1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/navernews/navernews/spiders\n",
            "/content/navernews/navernews/spiders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 크롤링을 진행할 메인 py 파일을 초기화(생성)\n",
        "#genspider [이름] 도메인 < 이름은 원하시는 이름으로 대신 위에서 생성한 프로젝트 이름과 겹치면 안됩니다. 도메인 초기화는 저도 아직 잘 모르겠네요.. 이걸 해줘야한다고 합니다.\n",
        "!scrapy genspider navernewsspider news.naver.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te5hCDVITMPX",
        "outputId": "8b7a7de0-851a-4565-cc23-6750fe54aa55"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created spider 'navernewsspider' using template 'basic' in module:\n",
            "  navernews.spiders.navernewsspider\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**여기까지 하시면 크롤링을 위한 밑작업은 끝났습니다. 밑에는 자동으로 생성된 파일들을 수정해서 크롤링하는 과정입니다.**"
      ],
      "metadata": {
        "id": "hy5PrRhPBtIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 폴더이름/폴더이름/items.py\n",
        "\n",
        "\"\"\"\n",
        "items.py는 scrapy 프로젝트를 만들면 기본으로 만들어지는 곳입니다.\n",
        "크롤링 한 정보를 담아둘 수납공간이라고 생각하시면 됩니다. \n",
        "기존에 크롤링했을때 사용한 변수라고 생각하시면 될 것 같아요.\n",
        "\"\"\"\n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class NavernewsItem(scrapy.Item):\n",
        "    source = scrapy.Field() # 신문사\n",
        "    category_1 = scrapy.Field() # 카테고리1\n",
        "    category_2 = scrapy.Field() # 카테고리2\n",
        "    title = scrapy.Field() # 제목\n",
        "    article = scrapy.Field() # 기사\n",
        "    reg_date = scrapy.Field() #등록 날짜\n",
        "    writer = scrapy.Field() # 기자\n"
      ],
      "metadata": {
        "id": "TeZqXAWI7fJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 폴더이름/폴더이름/settings.py\n",
        "\n",
        "\"\"\"\n",
        "저는 이부분이 처음에 True로 설정되어 있어서 계속 파싱을 못했습니다. \n",
        "ROBOTSTEXT_OBEY 값이 True일 경우 사이트에서 설정한 값을 확인하고 로봇이 접근을 못하게 했다면 접근을 안한다고 하네요.\n",
        "실제로 위 링크 들어가보면 네이버 뉴스는 로봇 접근을 막아놨습니다. (Disallow : \\ <이게 전부 막는다는 뜻이라고 합니다.)\n",
        "그런데 이걸 False로 바꿔주면 로봇이여도 접근이 가능하다고 합니다. 근데 가급적이면 막아놓은 곳은 사용 안하는게 좋다고 하네요. 자세한건 저도 좀 더 찾아봐야할 것 같아요.\n",
        "(사이트다마다 url끝에 /robots.txt를 붙여서 들어가서 확인하실 수 있습니다. ex. https://news.naver.com/robots.txt)\n",
        "\"\"\"\n",
        "ROBOTSTXT_OBEY = False\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "가져와야할 기사가 많아서 딜레이를  계속 줄여봤습니다. 저는 0.001로 했습니다.\n",
        "\n",
        "저는 USER-agent부분을 fake로 사용했습니다.\n",
        "!pip install scrapy-fake-useragent<< 실행하시고 밑에 DOWNLOADER_MIDDLEWARES 추가해주시면 정상 적용됩니다.\n",
        "\"\"\"\n",
        "\n",
        "DOWNLOAD_DELAY = 0.001\n",
        "\n",
        "DOWNLOADER_MIDDLEWARES = {\n",
        "    'scrapy.contrib.downloadermiddleware.useragent.UserAgentMiddleware': None,\n",
        "    'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware': None,\n",
        "    'scrapy_fake_useragent.middleware.RandomUserAgentMiddleware': 400,\n",
        "    'scrapy_fake_useragent.middleware.RetryUserAgentMiddleware': 401,\n",
        "}"
      ],
      "metadata": {
        "id": "S1AB9Kj28Lse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 폴더이름/폴더이름/spiders/크롤링 진행할 메인 파일.py\n",
        "\n",
        "import scrapy\n",
        "from navernews.items import NavernewsItem #위에서 사용할 Item의 공간을 미리 만들어놨으니 사용하기 위해서 불러오는 과정\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "import time\n",
        "import re\n",
        "\n",
        "# 각자 진행하는 카테고리에 맞게 수정하시면 됩니다. 코드는 직접 찾으셔야해요\n",
        "category = {\n",
        "  #경제\n",
        "  '101' : ['258','259','260','261','253','310','771'],\n",
        "  #IT/과학\n",
        "  '105' : ['226','227','228','229','230','283','731','732']\n",
        "}\n",
        "\n",
        "# 코드 매칭\n",
        "category_dict = {\n",
        "    '101' : '경제',\n",
        "    '105' : 'IT/과학',\n",
        "\n",
        "    #경제\n",
        "    '259' : '금융',\n",
        "    '258' : '증권',\n",
        "    '260' : '부동산',\n",
        "    '261' : '산업/제계',\n",
        "    '262' : '글로벌 경제',\n",
        "    '263' : '경제일반',\n",
        "    '310' : '생활경제',\n",
        "    '771' : '중기/벤처',\n",
        "\n",
        "    #IT/과학\n",
        "    '226' : '인터넷/SNS',\n",
        "    '227' : '통신/뉴미디어',\n",
        "    '228' : '과학/일반',\n",
        "    '229' : '게임/리뷰',\n",
        "    '230' : 'IT일반',\n",
        "    '283' : '컴퓨터',\n",
        "    '731' : '모바일',\n",
        "    '732' : '보안/해킹'\n",
        "    \n",
        "}\n",
        "\n",
        "\n",
        "# 크롤링 하는 클래스\n",
        "class naverNewsUrlSpider(scrapy.Spider):\n",
        "    name = \"naverNewsUrlSpider\"\n",
        "    allowed_domains = [\"news.naver.com\"]\n",
        "    start_urls = [\"http://news.naver.com/\"]\n",
        "        \n",
        "    # Scrapy로 크롤링을 실행시키면 이 함수가 실행됩니다. \n",
        "    def start_requests(self):\n",
        "\n",
        "      # 탐색할 날짜 리스트를 먼저 생성합니다.\n",
        "      start_date = datetime(2023, 2, 1)\n",
        "      end_date = datetime(2023, 3, 16)\n",
        "      dates = [start_date]\n",
        "      while True:\n",
        "          start_date = start_date + timedelta(days=1)\n",
        "          dates.append(start_date)\n",
        "          if start_date == end_date:\n",
        "              break\n",
        "      \n",
        "      # 날짜 -> 카테고리1 -> 카테고리2\n",
        "      for date in dates:\n",
        "        detail_date = date.strftime('%Y%m%d') #  20230201 형태\n",
        "        for main, sub_lst in category.items(): # 카테고리1 : 경제, IT/과학\n",
        "          for sub in sub_lst: # 카테고리2\n",
        "            url = f'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid2={sub}&sid1={main}&date={detail_date}&page=1'\n",
        "            yield scrapy.Request(url, self.url_parse, meta = {'page' : 1, 'urls' : [],  'main' : main, 'sub' : sub})\n",
        "            \n",
        "    # 보이지 않지만 현재 네이버 뉴스 -> 카테고리1 선택 화면입니다. 넘어온 상황\n",
        "    def url_parse(self, response):\n",
        "      # 10개의 기사가 출력되어있는 상황이니 url들을 가져옵니다.\n",
        "      urls = response.xpath('//*[@id=\"main_content\"]/div[2]/ul[1]/li/dl/dt[2]/a/@href').extract()\n",
        "\n",
        "      # url 목록이 이전 페이지와 같다면 종료.(재귀)\n",
        "      if response.meta.pop('urls') == urls:\n",
        "          return\n",
        "      \n",
        "      # url 목록에 있는 기사들을 하나씩 가져오도록 하는 부분입니다.\n",
        "      for url in urls:\n",
        "        yield scrapy.Request(url, self.parse, meta={**response.meta})\n",
        "\n",
        "      # 다음페이지(재귀)\n",
        "      page = response.meta.pop('page')\n",
        "      target_url = re.sub('page\\=\\d+', f'page={page+1}', response.url)\n",
        "      yield scrapy.Request(url=target_url, callback=self.parse_url, meta={**response.meta, 'page': page+1, 'urls':urls})\n",
        "\n",
        "\n",
        "    # 기사 본문 페이지를 읽는 부분. 원하시는 형태로 저장하시면 됩니다.\n",
        "    def parse(self, response):\n",
        "\n",
        "      item = NavernewsItem()\n",
        "      item['title'] = response.xpath('//*[@id=\"title_area\"]/span/text()').extract()\n",
        "      item['source'] = response.xpath('//*[@id=\"ct\"]/div[1]/div[1]/a/img[1]/@title').extract()\n",
        "      item['category_1'] = category_dict[response.meta.pop('main')]\n",
        "      item['category_2'] = category_dict[response.meta.pop('sub')]\n",
        "      item['article'] = response.xpath('//*[@id=\"dic_area\"]/text()').extract()\n",
        "      item['reg_date'] = response.xpath('//*[@id=\"ct\"]/div[1]/div[3]/div[1]/div/span/text()').extract()[0].split()[0]\n",
        "\n",
        "      yield item\n"
      ],
      "metadata": {
        "id": "Y-JYZjRS97nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ./navernews/navernews/spiders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WF9v-lyv61Sy",
        "outputId": "f3148986-243b-4c79-a52e-eabaf01240db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/navernews/navernews/spiders\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapy-fake-useragent"
      ],
      "metadata": {
        "id": "yMu5df6SAKQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL정보를 가져오는 클래스를 호출합니다.\n",
        "# scrapy crawl [클래스 이름] < 이 부분은 spiders폴더에서 진행했던 py폴더에서 class [이름] 이렇게 작성했던 이름을 써주시면 됩니다.\n",
        "# [-o '저장할 이름.csv -t csv] 이부분은 csv파일로 저장하기 위해서 추가했습니다.\n",
        "!scrapy crawl naverNewsUrlSpider -o test.csv -t csv"
      ],
      "metadata": {
        "id": "SlsU6NHlWRvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "len(pd.read_csv('test.csv'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLWWsWGiXv8o",
        "outputId": "7ef2deae-7059-4690-dbf7-52cf9f93372a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114645"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 결과물\n",
        "\n",
        "경제와 IT의 모든 기사 크롤링하는데 25분 걸렸습니다.\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/119479455/226164572-cc7820a0-cb2a-4214-8b6d-233d83dc9422.png)"
      ],
      "metadata": {
        "id": "7FUKSdQ0A1Ok"
      }
    }
  ]
}