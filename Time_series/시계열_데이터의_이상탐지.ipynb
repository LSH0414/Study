{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 시계열 데이터 이상탐지란\n",
        "\n",
        "이상치란 **'다른 방법에 의해 생성되었다는 의심이 될 정도로 관찰에서 너무 많이 벗어난 관찰이다.'** 따라서 이상값이란 예상되는 동작에 따르지 않는 관측치로 볼 수 있다.\n",
        "\n",
        "![Outlier](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Time-series-outliers.png?ssl=1)\n",
        "\n",
        "시계열에서 이상값은 두 가지 다른 의미를 갖는다. 원치않은 데이터일 경우와 특정 사건에 의한 데이터이다.\n",
        "\n",
        "</br></br>\n",
        "\n",
        "시계열 데이터에서 이러한 이상값 탐지에 대표적인 예시는 *사기 탐지*다. 주요 목표가 이상치 자체를 탐지하고 분석하는 것이다. 시계열에서 이상감지는 'Point Outlier', 'Subsequence Outlier' 두 유형으로 분류 할 수 있다.\n",
        "\n",
        "![Outlie2](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Time-series-outliers-types.png?resize=587%2C377&ssl=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "# Point Outlier\n",
        "Point Outlier는 시계열의 다른 값 또는 인접한 값들과 비교할 때 특정 시간에서 비정상적인 데이터를 말한다.\n",
        "\n",
        "![Point Outlier](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Point-outliers-in-time-series.png?resize=750%2C301&ssl=1)\n",
        "\n",
        "</br>\n",
        "\n",
        "# Subsequence Outlier\n",
        "Subsequence Outlier는 같은 움직임이 비정상적인 시간의 연속적인 현상을 의미한다. 전역 또는 로컬일 수 있으며 하나 또는 그 이상 시간 종속 변수에 영향을 줄 수도 있다.\n",
        "\n",
        "![Ss Outlier](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/%E2%80%8ASubsequence-outliers-in-time-series.png?ssl=1)\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "## 시계열 데이터의 이상 감지 기술\n",
        "\n",
        "### STL 분해\n",
        "STL(Seasonal and Trend decomposition using Loess) Loess는 비선형 관계를 추정하기 위한 기법이다. \n",
        "\n",
        "![STL](https://otexts.com/fppkr/fpp_files/figure-html/elecequip-stl2-1.png)\n",
        "\n",
        "\n",
        "STL에서는 t.window(Trend), s.window(Seasonal)의 매개변수에 따라 추세-주기와 계절성을 조정할 수 있다. \n",
        "\n",
        "</br></br>\n",
        "\n",
        "### 분류 및 회귀 트리(CART)\n",
        "\n",
        "의사결정 나무의 훌륭한 분류 기능을 통해 시계열 데이터에서의 이상값을 식별할 수 있다. 그 방법은\n",
        "\n",
        "- 의사결정 나무 모델에 이상과 비이상을 분류할 수 있는 포인트를 학습 시킬 수 있다. 이를 위해서 이상치 데이터에 대한 레이블링이 필요하고 이 데이터는 토이 데이터셋 외부에서는 쉽게 발견할 수 없어야 한다.\n",
        "\n",
        "- 학습시키지 않은 데이터도 필요하다. Isolation Forest 알고리즘을 사용하여 레이블이 지정된 데이터 집합의 도움 없이 특정 포인트가 이상치인지 아닌지 예측할 수 있다.\n",
        "\n",
        "핵심은 Isolation Forest는 다른 이상값 검색 방법과 달리 일반 데이터를 프로파일링 하는 대신에 명시적으로 이상치를 식별한다. Isolation Forest는 다른 앙상블 기법과 마찬가지로 의사결정 트리를 기반으로 한다. 즉, Isolation Forest는 약간의 다른 데이터 포인트라는 사실을 기반으로 이상치를 감지한다. 또한, 거리 또는 밀도 측정을 사용하지 않고 동작한다.\n",
        "\n",
        "- Isolation Forest 모델을 만들때 (contamination = outliers_fraction)옵션을 세팅하는데 이는 현재 데이터에 이상값의 비율을 모델에 알려주는 것입니다. 이는 trial/error 메트릭스이다.\n",
        "- 데이터에 대한 학습과 예측을 진행하면 정상에 대해서는 1을 이상치에 대해서는 -1을 반환한다.\n",
        "- 마지막으로 이상치를 시계열로 시각화한다.\n",
        "\n",
        "~~~\n",
        "plt.rc('figure',figsize=(12,6))\n",
        "plt.rc('font',size=15)\n",
        "catfish_sales.plot()\n",
        "~~~\n",
        "\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Time-series-anomalies.png?ssl=1)\n",
        "\n",
        "\n",
        "~~~\n",
        "#전처리\n",
        "outliers_fraction = float(.01)\n",
        "scaler = StandardScaler()\n",
        "np_scaled = scaler.fit_transform(catfish_sales.values.reshape(-1, 1))\n",
        "data = pd.DataFrame(np_scaled)\n",
        "\n",
        "# 모델 학습\n",
        "model =  IsolationForest(contamination=outliers_fraction)\n",
        "model.fit(data)\n",
        "\n",
        "\n",
        "catfish_sales['anomaly'] = model.predict(data)\n",
        "\n",
        "# 시각화\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "a = catfish_sales.loc[catfish_sales['anomaly'] == -1, ['Total']] #이상치\n",
        "ax.plot(catfish_sales.index, catfish_sales['Total'], color='black', label = 'Normal')\n",
        "ax.scatter(a.index,a['Total'], color='red', label = 'Anomaly')\n",
        "plt.legend()\n",
        "plt.show();\n",
        "~~~\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Anomaly-Detection-Isolation-Forest.png?ssl=1)\n",
        "\n",
        "그림을 보면 굉장히 아상치에 탐지를 잘 했음을 알 수 있다. 하지만 시자 지점에서의 이상치 탐지가 좋지 못한 결과같아 보인다. 이에 대한 이유는 두 가지가 있다.\n",
        "</br>\n",
        "- 처음에는 알고리즘이 이상치로 판단할 수 있는 데이터가 많지 않기 떄문인데 이는 많은 데이터를 얻을수록 더 많은 분산을 볼 수 있게 되고 스스로 그 범위를 조정하기 때문\n",
        "- 이런 값들이 많이 보인다면 모델의 contamination 파라미터를 너무 높게 설정했기 때문이다.\n",
        "</br>\n",
        "\n",
        "</br>\n",
        "\n",
        "**장점**</br>\n",
        "이 기술의 가장 큰 이점은 더욱 정교한 모델을 만들기 위해서 원하는 만큼 많은 변수나 피처를 넣을 수 있다는 것입니다.\n",
        "\n",
        "\n",
        "**단점**</br>\n",
        "단점은 피처가 증가함에 따라서 컴퓨터 퍼포먼스에 영향을 준다. 이럴 경우 피처 선택을 신중하게 해야합니다.\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "### 예측을 사용한 이상치 탐지\n",
        "미래 예측을 통한 이상치 감지는 과거의 데이터에 white noise를 추가하여 미래에 대한 예측을 생성하는 접근 방식이다. 미래에 대한 예측에 대한 선은 부드러운 형태일 것입니다.</br>\n",
        "이 방법을 사용할때 어려운 점은 차이의 수, 자기회귀 계수, 예측 오류 계수를 선택해야한다는 것입니다.\n",
        "</br>\n",
        "이는 새로운 신호의 작업을 할때마다 새로운 예측 모델을 만들어야합니다.\n",
        "</br>\n",
        "또 다른 장애요소는 차이를 확인한 후 신호가 고정되어 있어야 한다는 것입니다. 간단히 말해, 신호가 시간에 의존해서는 안됩니다. 이는 중요한 제약 조건입니다.\n",
        "</br>\n",
        "이동평균(Moving Average), 자기회귀(Autoregress) 접근 방식 및 ARIMA와 같은 방법으로 활용할 수 있습니다. ARIMA를 통한 이상 탐지 방법은 아래와 같다.\n",
        "- 과거 데이터를 통해 예측과 훈련 데이터와의 크기 차이가 있다.\n",
        "- 임계값(theshold)를 설정하고 임계값과 다름을 기준으로 이상치를 식별한다.\n",
        "</br>\n",
        "\n",
        "이를 위해 Prophet 모듈을 사용합니다.\n",
        "\n",
        "~~~\n",
        "def fit_predict_model(dataframe, interval_width = 0.99, changepoint_range = 0.8):\n",
        "   m = Prophet(daily_seasonality = False, yearly_seasonality = False, weekly_seasonality = False,\n",
        "               seasonality_mode = 'additive',\n",
        "               interval_width = interval_width,\n",
        "               changepoint_range = changepoint_range)\n",
        "   m = m.fit(dataframe)\n",
        "   forecast = m.predict(dataframe)\n",
        "   forecast['fact'] = dataframe['y'].reset_index(drop = True)\n",
        "   return forecast\n",
        "\n",
        "pred = fit_predict_model(t)\n",
        "~~~\n",
        "\n",
        "아래 코드는 Prophet에서 제공하는 interval_width를 가져오는 과정입니다.\n",
        "\n",
        "~~~\n",
        "def detect_anomalies(forecast):\n",
        "   forecasted = forecast[['ds','trend', 'yhat', 'yhat_lower', 'yhat_upper', 'fact']].copy()\n",
        "forecasted['anomaly'] = 0\n",
        "   forecasted.loc[forecasted['fact'] > forecasted['yhat_upper'], 'anomaly'] = 1\n",
        "   forecasted.loc[forecasted['fact'] < forecasted['yhat_lower'], 'anomaly'] = -1\n",
        "#anomaly importances\n",
        "   forecasted['importance'] = 0\n",
        "   forecasted.loc[forecasted['anomaly'] ==1, 'importance'] =\n",
        "       (forecasted['fact'] - forecasted['yhat_upper'])/forecast['fact']\n",
        "   forecasted.loc[forecasted['anomaly'] ==-1, 'importance'] =\n",
        "       (forecasted['yhat_lower'] - forecasted['fact'])/forecast['fact']\n",
        "\n",
        "   return forecasted\n",
        "pred = detect_anomalies(pred)\n",
        "~~~\n",
        "\n",
        "이를 시각화하면 아래와 같습니다.\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Anomaly-detection.png?ssl=1)\n",
        "\n",
        "</br></br>\n",
        "\n",
        "**장점**</br>\n",
        "이 방법은 월간, 연간과 같은 다양한 계절성 매개변수에 대해서 훌륭한 결과를 보여줍니다. Isolation Forest와 비교한다면 엣지 케이스에 대한 처리가 좋습니다.\n",
        "\n",
        "</br>\n",
        "**단점**</br>\n",
        "이 방법은 예측을 기반으로 하기 때문에 제한된 데이터를 갖고 있는 경우 어려움이 있습니다. 제한된 데이터는 곧 예측 품질이 낮아지고 이상 감지 정확도가 낮이지기 떄문입니다.\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "### 클러스터링 기반 이상 탐지\n",
        "클러스트링 기반 이상 탐지는 Isolation Forest와 같은 비지도 학습 기술이다.\n",
        "\n",
        "간단하게 정의된 클러스터(집단) 외부에 있는 데이터를 잠재적인 이상치라 정의합니다.\n",
        "\n",
        "k-mean 기법으로 분류를 해볼텐데 그러기 위해서 우선 처리할 클러스터 수를 알아야 합니다. 이때 Elbow Method가 매우 효율적입니다.\n",
        "\n",
        "Elbow Method는 클러스터 수 vs 분산 설명/목표/점수의 그래프입니다.\n",
        "\n",
        "~~~\n",
        "data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]\n",
        "n_cluster = range(1, 20)\n",
        "kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]\n",
        "scores = [kmeans[i].score(data) for i in range(len(kmeans))]\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "ax.plot(n_cluster, scores)\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Elbow Curve')\n",
        "plt.show();\n",
        "~~~\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Elbow-method.png?ssl=1)\n",
        "\n",
        "10개의 군집에서 어느정도 안저적인 수평을 형성합니다. 따라서 클러스터 수를 10개로 결정합니다.\n",
        "\n",
        "다음은 유지할 피처를 선정해야합니다.\n",
        "\n",
        "~~~\n",
        "data = df[['price_usd', 'srch_booking_window', 'srch_saturday_night_bool']]\n",
        "X = data.values\n",
        "X_std = StandardScaler().fit_transform(X)\n",
        "#Calculating Eigenvecors and eigenvalues of Covariance matrix\n",
        "mean_vec = np.mean(X_std, axis=0)\n",
        "cov_mat = np.cov(X_std.T)\n",
        "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
        "# Create a list of (eigenvalue, eigenvector) tuples\n",
        "eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
        "eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
        "# Calculation of Explained Variance from the eigenvalues\n",
        "tot = sum(eig_vals)\n",
        "var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
        "cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(len(var_exp)), var_exp, alpha=0.3, align='center', label='individual explained variance', color = 'y')\n",
        "plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
        "plt.ylabel('Explained variance ratio')\n",
        "plt.xlabel('Principal components')\n",
        "plt.legend(loc='best')\n",
        "plt.show();\n",
        "~~~\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Anomaly-detection-components.png?ssl=1)\n",
        "\n",
        "첫 피처가 분산의 50%를 설명해주고 있습니다. 두번째는 30%넘게 설명하고 있습니다. 첫 2개의 피처가 80%의 정보를 포함하고 있습니다. 따라서 n_components를 2로 설정합니다. 여기서 중요한 점은 무시할만한 피처는 없다는 것을 잊어서 안됩니다.\n",
        "\n",
        "클러스터링 기반의 이상 탐지의 기본 가정은 데이터를 클러스팅 한 결과에서 정상 데이터는 클러스터에 속하고 이상치는 어떤 클러스테에도 속하지 않거나 작은 클러스터에 속한다는 것입니다.\n",
        "\n",
        "아래와 같은 방법으로 시각화를 합니다.\n",
        "1. 각 점과 가장 가까운 중심 사이의 거리를 계산 합니다. 가장 큰 거리는 이상치로 간주합니다.\n",
        "2. Isolation Forest와 유사하게 데이터셋에 존재하는 이상값의 비율에 대한 정보를 제공해야합니다.(outliers_fraciton) 이 하이퍼파라미터는 hit/trial 혹은 grid-search로 설정해야하는 변수입니다. 보통 시작 수치는 0.1로 추정합니다.\n",
        "3. outliers_fraction을 사용하여 이상치 수를 계산합니다.\n",
        "4. 임계값(threshold)를 이상치의 최소 거리로 설정합니다.\n",
        "5. 'anomaly1'에 결과는 0:정상, 1:이상치로 나타납니다.\n",
        "\n",
        "\n",
        "~~~\n",
        "# return Series of distance between each point and its distance with the closest centroid\n",
        "def getDistanceByPoint(data, model):\n",
        "   distance = pd.Series()\n",
        "   for i in range(0,len(data)):\n",
        "       Xa = np.array(data.loc[i])\n",
        "       Xb = model.cluster_centers_[model.labels_[i]-1]\n",
        "       distance.at[i]=np.linalg.norm(Xa-Xb)\n",
        "   return distance\n",
        "outliers_fraction = 0.1\n",
        "# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly\n",
        "distance = getDistanceByPoint(data, kmeans[9])\n",
        "number_of_outliers = int(outliers_fraction*len(distance))\n",
        "threshold = distance.nlargest(number_of_outliers).min()\n",
        "# anomaly1 contain the anomaly result of the above method Cluster (0:normal, 1:anomaly)\n",
        "df['anomaly1'] = (distance >= threshold).astype(int)\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "colors = {0:'blue', 1:'red'}\n",
        "ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df[\"anomaly1\"].apply(lambda x: colors[x]))\n",
        "plt.xlabel('principal feature1')\n",
        "plt.ylabel('principal feature2')\n",
        "plt.show();\n",
        "~~~\n",
        "\n",
        "이상치로 분류된 데이터를 실제값과 비교해봅니다.\n",
        "\n",
        "~~~\n",
        "df = df.sort_values('date_time')\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "a = df.loc[df['anomaly1'] == 1, ['date_time', 'price_usd']] #anomaly\n",
        "ax.plot(pd.to_datetime(df['date_time']), df['price_usd'], color='k',label='Normal')\n",
        "ax.scatter(pd.to_datetime(a['date_time']),a['price_usd'], color='red', label='Anomaly')\n",
        "ax.xaxis_date()\n",
        "plt.xlabel('Date Time')\n",
        "plt.ylabel('price in USD')\n",
        "plt.legend()\n",
        "fig.autofmt_xdate()\n",
        "plt.show()\n",
        "~~~\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Visualize-anomalies-dataframe.png?ssl=1)\n",
        "\n",
        "결과는 약간의 실측이 있었지만 peak(꼭지점)의 값들을 잘 잡아냈습니다. 이런 문제의 일부는 outlier_fraction에 대한 값을 수정하지 않았기 때문일 수 있습니다.\n",
        "\n",
        "</br></br>\n",
        "\n",
        "**장점**\n",
        "이 기술의 가장 큰 장점은 다른 비지도 학습과 비슷하게 더욱 정교한 모델을 만들기 위해 많은 피처들을 사용할 수 있습니다.\n",
        "\n",
        "**단점**\n",
        "피처의 수가 늘어날 수록 컴퓨터의 성능에 빠르게 영향을 줄 수 있습니다. 게다가 조정해야할 하이퍼파라미터들이 많기 때문에 성능에 대한 변동 가능성이 높습니다.\n",
        "\n",
        "</br></br></br>\n",
        "\n",
        "### 오토인코더(Autoencoders)\n",
        "딥러닝 기술인 오토인코더를 사용하여 이상치 처리하는 방법이 있습니다.\n",
        "\n",
        "오토인코더는 다른 차원을 통해 피처를 추출하면서 입력 데이터를 재생성하는 비지도 학습 기술입니다. 다시말해 오토인코더의 데이터 형태인 Latent Representation을 사용하여 차원 축소하는 것입니다.\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Anomaly-detection-autoencoders.png?ssl=1)\n",
        "\n",
        "**이상탐지에 차원 축소를 하는 이유**</br>\n",
        "차원을 줄이면 이상치를 포함한 일부 정보가 손실되는것이 아닌가라는 의문이들 수 있습니다. 하지만 데이터의 주패턴이 식별되면 이상값은 자연스럽게 드러난다는 것입니다. 많은 거리 기반의 기술(ex. Knn)은 모든 피처 차원에서 모든 데이터의 거리를 계산할때 차원의 저주에 고통받습니다. 따라서 높은 차원을 줄여야 합니다.\n",
        "\n",
        "흥미롭게도 차원을 축소하는 과정에서 이상치가 식별됩니다. 이상치 감지는 차원 축소의 부산물이라 할 수 있습니다.\n",
        "\n",
        "오토인코더는 이상치를 찾는 비지도 학습 접근법입니다.\n",
        "\n",
        "</br>\n",
        "\n",
        "\n",
        "**왜 오토인코더인가?**</br>\n",
        "이상치 탐지를 위한 PCA(Principal Component Analysis)가 있지만 왜 오토인코더가 필요할까요? 그 이유는 PCA는 선형 대수를 사용하여 변환하기 때문입니다. 대조적이게 오토인코더는 비선형 활성화 함수와 다중 계층을 사용하여 비선형 변환을 수행합니다. PCA로 하나의 거대한 변환으로 훈련시키는 것보다 오토인코더로 여러 계층으로 훈련시키는 것이 더 효율 적입니다. 오토인코더는 데이터 문제가 본질적으로 복잡하고 비선형일때 장점을 보여줍니다.\n",
        "\n",
        "이제 오토인코더를 활용한 모델을 구축해봅시다.\n",
        "'Tensorflow'와 'Pytorch'를 활용하여 오토인코더를 구현할 수 있습니다. 이번에는 간단한 구현이 목적이기에 PyOD라는 모듈을 사용하여 약간의 입력으로 오토인코더를 구현해봅니다.\n",
        "\n",
        "\n",
        "~~~\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pyod.models.auto_encoder import AutoEncoder\n",
        "from pyod.utils.data import generate_data\n",
        "contamination = 0.1  # percentage of outliers\n",
        "n_train = 500  # number of training points\n",
        "n_test = 500  # number of testing points\n",
        "n_features = 25 # Number of features\n",
        "X_train, y_train, X_test, y_test = generate_data(\n",
        "   n_train=n_train, n_test=n_test,\n",
        "   n_features= n_features,\n",
        "   contamination=contamination,random_state=1234)\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "~~~\n",
        "\n",
        "비지도 학습을 처리할때 항상 표준화를 해주는 것이 안전한 방법입니다.\n",
        "\n",
        "~~~\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_train = StandardScaler().fit_transform(X_train)\n",
        "X_train = pd.DataFrame(X_train)\n",
        "X_test = StandardScaler().fit_transform(X_test)\n",
        "X_test = pd.DataFrame(X_test)\n",
        "~~~\n",
        "\n",
        "\n",
        "훈련시킬 값들을 PCA로 2차원으 플로팅시켜 시각화 해봅니다.\n",
        "\n",
        "~~~\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(2)\n",
        "x_pca = pca.fit_transform(X_train)\n",
        "x_pca = pd.DataFrame(x_pca)\n",
        "x_pca.columns=['PC1','PC2']\n",
        "cdict = {0: 'red', 1: 'blue'}\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(X_train[0], X_train[1], c=y_train, alpha=1)\n",
        "plt.title('Scatter plot')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.show()\n",
        "~~~\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Anomaly-detection-scatter-plot.png?ssl=1)\n",
        "\n",
        "검은색이 정상값, 노란색이 이상치입니다.\n",
        "\n",
        "1. 모델 구축\n",
        "~~~\n",
        "clf = AutoEncoder(hidden_neurons =[ 25 , 2 , 2 , 25 ])\n",
        "clf.fit(X_train)\n",
        "~~~\n",
        "\n",
        "2. cut point 설정\n",
        "훈련된 모델을 통해 테스트 데이터에서 이상치 점수를 예측해봅니다. 여기서 이상치 점수는 멀리 떨어진 거리로 계산이됩니다. PyOD함수의 .decision_function()은 각 데이터 지점에 대한 거리(이상치 점수)를 계산합니다.\n",
        "\n",
        "~~~\n",
        "# Get the outlier scores for the train data\n",
        "y_train_scores = clf.decision_scores_\n",
        "# Predict the anomaly scores\n",
        "y_test_scores = clf.decision_function(X_test)  # outlier scores\n",
        "y_test_scores = pd.Series(y_test_scores)\n",
        "# Plot it!\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(y_test_scores, bins='auto')\n",
        "plt.title(\"Histogram for Model Clf1 Anomaly Scores\")\n",
        "plt.show()\n",
        "~~~\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Anomaly-detection-histogram.png?ssl=1)\n",
        "\n",
        "위 그림은 히스토그램을 통해 이상점수에 대한 빈도를 계산한 것입니다. 높은 점수들이 빈도가 적은것을 확인 할 수 있고 cut point를 4.0으로 선택하는 것이 합당할 것으로 보입니다.\n",
        "\n",
        "3. 각 클러스테의 통계치 가져오기\n",
        "cut point인 4.0 미만의 클러스터를 0, 4.0이상인 클러스터를 1로 할당합니다. 그룹화를 통해 클러스터별 요약 통계를 계산합니다. \n",
        "\n",
        "~~~\n",
        "df_test = X_test.copy()\n",
        "df_test['score'] = y_test_scores\n",
        "df_test['cluster'] = np.where(df_test['score']<4, 0, 1)\n",
        "df_test['cluster'].value_counts()\n",
        "df_test.groupby('cluster').mean()\n",
        "~~~\n",
        "\n",
        "아래의 결과는 각 클러스터으 평균 변수 값을 나타냅니다. 높은 점수는 표준에서 멀리 떨어져 있음을 뜻합니다.\n",
        "\n",
        "![img](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Anomaly-detection-cluster.png?ssl=1)\n",
        "\n",
        "</br></br>\n",
        "\n",
        "**장점**</br>\n",
        "1. 자동인코더는 고차원 데이터를 쉽게 처리합니다.\n",
        "2. 비선형 방법, 고차원 데이터 세트 내의 복잡한 패턴 파악이 가능합니다.\n",
        "\n",
        "**단점**</br>\n",
        "1. 딥 러닝 기반의 방법이기에 데이터가 적을 경우 큰 어려움이 있습니다.\n",
        "2. 깊은 네트워크가 있는 빅데이터를 다루면 계산 비용이 상상 이상으로 커집니다.\n"
      ],
      "metadata": {
        "id": "diUEQLH68fgp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W79hI4bWR1q4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}